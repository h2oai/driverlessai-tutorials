{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driverless AI: Handling Imbalanced Data\n",
    "\n",
    "This notebook provides an example of using Driverless AI to train a model to predict an imbalanced target. The data used in this notebook is the [UCI Bank Marketing Dataset](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing).  The goal is to predict whether or not the outcome of the marketing call would be the client subscribing to the bank term deposit.  Only about 4% of clients end up subscribing.\n",
    "\n",
    "When a dataset has an imbalanced target column, there can be improvement in performance by over-sampling the minority class or under-sampling the majority class.  Driverless AI will automatically under-sample the majority class if the data is considered large and imbalanced (see: http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/data-sampling.html).  Driverless AI will not perform any imbalanced sampling unless the dataset needs to be sampled down because of its large size.  \n",
    "\n",
    "If a user would like to increase the weight or importance of a particular class and would like a high level of control, they can add a weight column.  A weight column tells the Driverless AI models how important a particular row is when calculating the error of the model.  If one row is given a weight of 2 and all other rows are given a weight of 1, then the row with weight of 2 is considered twice as important.  The weight column can act as a way of over-sampling the minority class.  For example, by creating a weight column where the weight for all of our subscription clients is 10 and everyone else is 1, we are essentially copying our subscription clients 10 times or over-sampling 10 times.\n",
    "\n",
    "The goal of this notebook is to programmatically figure out the best weight column by evaluating how an experiment with a specific weight column performs on our test data.  The test data will not be given a weight column so we are comparing the un-weighted performance metrics on the test data.\n",
    "\n",
    "\n",
    "## Notes:\n",
    "\n",
    "* This is an early release of the Driverless AI Python client.\n",
    "* Python 3.6 is the only supported version.\n",
    "* You must install the `h2oai_client` wheel to your local Python. This is available from the RESOURCES link in the top menu of the UI.\n",
    "\n",
    "![py-client](images/py_client_link.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Steps\n",
    "\n",
    "1. Sign in\n",
    "2. Import Data and Decide on Weights to Use\n",
    "3. Launch Driverless AI Experiments for Different Weights (over-weighting the minority class) and Link to Project\n",
    "4. Compare Experiments in the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sign In\n",
    "\n",
    "Import the required modules and log in.\n",
    "\n",
    "Pass in your credentials through the Client class which creates an authentication token to send to the Driverless AI Server. In plain English: to sign into the Driverless AI web page (which then sends requests to the Driverless Server), instantiate the Client class with your Driverless AI address and login credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2oai_client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'http://ip_where_driverless_is_running:12345'\n",
    "username = 'username'\n",
    "password = 'password'\n",
    "h2oai = Client(address = address, username = username, password = password)\n",
    "# make sure to use the same user name and password when signing in through the GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Data and Determine Weights\n",
    "\n",
    "We will start by using Pandas to import the data into Python.  We will then evaluate how imbalanced the data is and use that information to guide us on what weights to use for our minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_pd = pd.read_csv(\"./bank-additional-full.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     19086\n",
       "yes      913\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd[\"y\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only about 4% of clients end up subscribing.  In order for the two classes to be evenly distributed, the `yes` class needs to occur about 20 times more often.  Therefore, we will choose the following weights to evaluate: \n",
    "\n",
    "* weight = 1: no weight done (this is our baseline)\n",
    "* weight = 2: equivalent to doubling the `yes` class\n",
    "* weight = 5: equivalent to duplicating the `yes` class 5 times\n",
    "* weight = 10: equivalent to duplicating the `yes` class 10 times\n",
    "* weight = 20: equivalent to duplicating the `yes` class 20 times (this would mean both `yes` and `no` occur about the same number of times in the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Launch Driverless AI Experiments for Different Weights\n",
    "\n",
    "We will use a for loop to train an experiment for each weight setting.  We will then calculate how well this experiment does on our un-weighted test data.  The idea is that by weighting the minority class more highly, perhaps we will create a model that performs better overall on our un-weighted test data. \n",
    "\n",
    "We will first split the data into train and test and then create a project that will contain all our experiments for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df_pd, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Project\n",
    "project_key = h2oai.create_project(\"campaign_weights\", \"effects of weighting for our campaign use case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will upload the test dataset.  The test dataset will not have a weight column because we want to see how the Driverless AI experiment performs on un-weighted hold out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Test Data\n",
    "# We will not give any weight \n",
    "test.to_csv(\"./test.csv\", index=False)\n",
    "test_path = './test.csv'\n",
    "test_dai = h2oai.upload_dataset_sync(test_path)\n",
    "\n",
    "# Link Test Data to Project\n",
    "h2oai.link_dataset_to_project(project_key = project_key, dataset_key = test_dai.key, dataset_type = \"Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our for loop will: \n",
    "\n",
    "1. Add a weight column to the training dataset (over-weighting the minority class)\n",
    "2. Link the training data to our project\n",
    "3. Launch an experiment with the weighted training data\n",
    "4. Save the unweighted test MCC to a dataframe for later comparison.\n",
    "\n",
    "Note: For demo purposes the accuracy and time of these experiments is set to 1.  Consider increasing these values for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments with different weight columns\n",
    "weights = [1, 2, 5, 10, 20]\n",
    "results = pd.DataFrame()\n",
    "for weight in weights:\n",
    "    # Add weight column to train\n",
    "    weighted_train = train.copy()\n",
    "    weighted_train[\"weight\"] = 1\n",
    "    weighted_train.loc[weighted_train[\"y\"] == \"yes\", \"weight\"] = weight\n",
    "    weighted_train.to_csv(\"./campaign_train-weighted.csv\", index=False)\n",
    "    train_weighted_dai = h2oai.upload_dataset_sync(\"./campaign_train-weighted.csv\")\n",
    "    \n",
    "    # Link Train Data to Project\n",
    "    h2oai.link_dataset_to_project(project_key = project_key, \n",
    "                                  dataset_key = train_weighted_dai.key, \n",
    "                                  dataset_type = \"Training\")\n",
    "    \n",
    "    # Launch Experiment\n",
    "    experiment = h2oai.start_experiment_sync(dataset_key=train_weighted_dai.key,\n",
    "                                         testset_key = test_dai.key,\n",
    "                                         target_col=\"y\",\n",
    "                                         is_classification=True,\n",
    "                                         accuracy=1,\n",
    "                                         time=1,\n",
    "                                         interpretability=1,\n",
    "                                         scorer=\"MCC\",\n",
    "                                         seed = 1234,\n",
    "                                         weight_col = \"weight\")\n",
    "    \n",
    "    # Update Experiment Description\n",
    "    h2oai.update_model_description(experiment.key, \"weight: \" + str(weight))\n",
    "    \n",
    "    # Link Experiment to Project\n",
    "    h2oai.link_experiment_to_project(project_key = project_key, \n",
    "                                     experiment_key = experiment.key)\n",
    "    \n",
    "    # Save the un-weighted Test MCC to our results frame\n",
    "    exp_results = pd.DataFrame([{'weight': weight, 'test_score': experiment.test_score}])\n",
    "    results = pd.concat([results, exp_results], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Experiments in the Project\n",
    "\n",
    "The table below shows the un-weighted MCC on the test data for the different weights we supplied.  We can see that the highest MCC occurs if we weight the minority class as twice as important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_score</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.550519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.557476</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.537705</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.544846</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.532728</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_score  weight\n",
       "0    0.550519       1\n",
       "0    0.557476       2\n",
       "0    0.537705       5\n",
       "0    0.544846      10\n",
       "0    0.532728      20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results in Project\n",
    "\n",
    "We can use the Projects page in the UI to also compare the performance.  Below we are comparing the experiments by their Test MCC.\n",
    "\n",
    "![](./images/weighted_project.png)\n",
    "\n",
    "\n",
    "We can also compare 3 experiments at a time next to each other.  Here we are comparing the top 3 experiments based on their Test MCC and viewing the differences in their experiment summary and variable importance.\n",
    "\n",
    "\n",
    "![](./images/compare_weighted_experiments.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
