{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word Embeddings\n",
    "\n",
    "From Driverless AI version 1.7.0, text models can take in pretrained word embeddings through expert settings. There are several pre-trained word embeddings available in the open source domain like [Glove](https://nlp.stanford.edu/projects/glove/) and [Fasttext](https://fasttext.cc/docs/en/crawl-vectors.html). We can download these embeddings and use them in our models. These embeddings are trained on corpus like wikipedia, common crawl etc. \n",
    "\n",
    "We can also train our own embeddings on our domain dataset instead of using the publicly available ones. This one is particularly useful when there is a good amount of text data that is not tagged and want to use that information. This notebook is to help create custom pre-trained embeddings.\n",
    "\n",
    "The data used in this example is [US Airline Sentiment dataset](https://www.figure-eight.com/wp-content/uploads/2016/03/Airline-Sentiment-2-w-AA.csv) from [Figure Eight’s Data for Everyone](https://www.figure-eight.com/data-for-everyone/) library. The dataset is split into training and test with this [simple script](https://gist.github.com/woobe/bd79d9f4d7ea139c5d2eb4cf1de1e7db) and the train file is used for word embeddings creation. Please use your own text corpus inplace of this airline train file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please enter the file name\n",
    "file_name = \"train_airline_sentiment.csv\"\n",
    "# Please enter the name of the text column\n",
    "col_name = \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the h2o module and H2OWord2vecEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"11.0.1\" 2018-10-16; OpenJDK Runtime Environment 18.9 (build 11.0.1+13); OpenJDK 64-Bit Server VM 18.9 (build 11.0.1+13, mixed mode)\n",
      "  Starting server from /Users/srk/envs/DS2/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/db/49r_20s91bg8qhg08qf78x100000gn/T/tmp8m3vtkx0\n",
      "  JVM stdout: /var/folders/db/49r_20s91bg8qhg08qf78x100000gn/T/tmp8m3vtkx0/h2o_srk_started_from_python.out\n",
      "  JVM stderr: /var/folders/db/49r_20s91bg8qhg08qf78x100000gn/T/tmp8m3vtkx0/h2o_srk_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Kolkata</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.4</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 month and 24 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_srk_z7y5eb</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>4 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>12</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>12</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       Asia/Kolkata\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.4\n",
       "H2O cluster version age:    1 month and 24 days\n",
       "H2O cluster name:           H2O_from_python_srk_z7y5eb\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    4 Gb\n",
       "H2O cluster total cores:    12\n",
       "H2O cluster allowed cores:  12\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "h2o.init()\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset file. Please note that the input file should be a csv file with a valid header in the first line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "df = h2o.import_file(file_name, header=1, sep=\",\")\n",
    "df = df[[col_name]].ascharacter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    # tokenize the sentences\n",
    "    tokenized = sentences.tokenize(\"\\\\W+\")\n",
    "    # lower case the text column\n",
    "    tokenized = tokenized.tolower()\n",
    "    # filter out the sentences which has less than 2 characters or where text is missing\n",
    "    tokenized = tokenized[(tokenized.nchar() >= 2) | (tokenized.isna()),:]\n",
    "    return tokenized\n",
    "\n",
    "words = tokenize(df[col_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build the word2vec model. We can also adjust the parameters of the word2vec mdoel. Please refer to the [documentation of H2oWord2vecEstimator](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html#h2oword2vecestimator) for more details on the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build word2vec model\n",
      "word2vec Model Build progress: |██████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "print(\"Build word2vec model\")\n",
    "w2v_model = H2OWord2vecEstimator(min_word_freq=3,\n",
    "                                 vec_size=300,\n",
    "                                 window_size=5,\n",
    "                                 epochs=10,\n",
    "                                 word_model=\"skip_gram\")\n",
    "w2v_model.train(training_frame=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the word embeddings as text file. \n",
    "\n",
    "This file can be given as pre-trained word embedding input for Driverless AI. The option is present in `Expert Settings -> NLP -> Path to pretrained embeddings for TensorFlow NLP models` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.to_frame().as_data_frame().to_csv(\"w2vec.txt\", float_format='%.6f', sep=\" \", header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
